gpt-4o-mini-high: ensure eval script does not use any tools
claude 3.7-reasoning
gemini 2.5-pro (need to get key)


10:00: gather thoughts. look into zebralogic
11:30: hopefully zebralogic is finished. write the DP programs
12:00: hopefully have some traction on DP programs
2:30: hopefully mostly done
3:00: hopefully have a dataset to present. still need to calibrate difficulty to qwen performance. HAVE A PLAN FOR WHEN YOU ARE EVALUATING. DO NOT REST NEEDLESSLY
3:15: clean up perpendicular intersection problems
3:30: begin working on my problems. start with polygon and basic.
4:30: optimistically, polygon and basic will be done and all issues with those wrapped up. begin evaluation on triangle and circle-line problems

DO NOT DECLARE VICTORY TOO EARLY. only done when you are done.







check frontier model performance on 12-gon problems (for fun):

1. Suppose you have a 9-gon, with vertices numbered 1 through 9 in counterclockwise order.

Draw the diagonal from vertex 1 to vertex 5. Then draw the diagonal from vertex 3 to vertex 8. Then, rotate the entire setup, including the constructed diagonals, 5 vertices counterclockwise (so that vertex 1 ends up where vertex 6 was), and superimpose it on the original (so that the resulting diagram contains both the original diagonals and the rotated versions of the diagonals). The original 9-gon will be partitioned into a collection of smaller polygons. How many such polygons will there be? (Only count the "smallest" polygons, i.e, those that do not have any of the diagonals running through them; this is not a combinatorics problem.) 
(answer: 9)

2.  That's correct. Of these, how many will have exactly 4 sides? 
(answer: 6)

3.  Find the number of unique rectangles that can be formed inside a fixed regular dodecagon (12-gon) where each side of the rectangle lies on either a side or a diagonal of the dodecagon.

Note that it is possible for a rectangle to be contained within another rectangle, and that the rectangles may not extend beyond the boundaries for the 12-gon. 
(answer: 315)

-deepseek-r1:
1. correct, 2. incorrect (confused the whole way)

-claude 3.7-reasoning:
1. correct only the 2nd time, 2. incorrect twice (mistakes, using valid method of analysis of planar graph, i think does not generalize to harder questions)
3. incorrect (but valid reasoning about the classes of line orientations, and identifying perpendicular pairs)

-gpt-4o:
1. correct, 2. correct, 3. correct (using python and shapely lib)

-gemini-2.5-pro:
1. correct, 2. correct, 3. incorrect (some valid reasoning about the classes of line orientations, mistakes made later on)


Note shapely does what you wanted for the polygon intersection problems.

When eval-ing on my exploratory problems, need to ensure that the problems are well-founded

Explorative
For all subfields for this, the deliverable is a set of data:
1000 train/"test-in", i.e, val, from the same distribution. they should be parameterized by some factor which means that qwen-32b-Instruct has a ~10 percent success rate at them.
so that we can eval fine tuning performance on them, and also eval performance out of the distribution.
We need one of these for each geometry subfield.
Best if I can classify the problems into ~6 categories instead of just the 4-ish I have. (Just split by classification type)
    - fix angles by maybe only rotating by quantities that are interesting
    - fix measure angle also by only measuring an angle when it is interesting, and also not pi

Also need multiple logic domains. There is zebra-logic also.

Compositional
(what is the format these things are expected to be found in?)
[x] Geometry-combinatorics thing: make custom dataset for geometry FT
- test set made by hand
    - still have more ideas if needed: hexagon grid
 
- Remember "circle growing" or "shape moving" type problems that cause the other constraints on a problem to change. "Five good templates" or similar.

- Third compositional problem is needed. We want to not have to make a new training set for this, so ideally we take advantage of circle and line constructions to form the eval set.
Spiral problem is likely another problem; be clear on how this breaks down


Transformational
- implement logic problems, i.e, gridworld stuff
- Implement geometry circles problem/write variations


Other (project related remarks)
Need to generate eval set.
Need to understand the structure of the existing repo a bit better-- what is
Should create a visualizer. Or no matter what, we need a diagram of some kind to explain the three categories.
Best if a similar problem can be used to explain all three. To that end, I think making a circles-and-lines type diagram would be good,



formatting  the code is a more trivial matter, just has to look good for submission



class of rectangles for the polygon rotation problems (simply check for right angles)

create polygon rotation problems
instead of just asking how many polygons, ask how many 4-gons or how many triangles or whatever


(more like instead of writing tests, should just make more atomic changes)
(maybe should've asked cursor to do a more test-driven development approach early on)
(also should do things more breadth-first)


cursor hotkeys:
    - bash: ctrl-y to paste. ctrl-/ to under
    - focus chat hotkey? ctrl-y taken by bash
vim keys:
    - see :help g
    - ge to get to end of previous word
    - gi to edit at last edit position
    - what is visualstar?
    - https://filiphalas.com/keyboard-controlled-vscode-with-vim-and-tmux
        - see tmux, sneak, etc. settings, substitution, surround
    - C-d now does something weird ('visual multi-cursor') that doesn't work with a correctly

- fixed translation errors:
    - arg order in mirror commands
    - unhandled triangle retvals for the new segments

- create simple script to analyze problem output distribution
- grade problems by the 100s to see output difficulty distribution
- visualize problems

- can do 

- high level agenda:
- fix (some of) the dumb things
[x] do restricted problem generation
- fix dumb things in iteration if needed
    [x] ban making more than one point that is just floating
    [x] ban making point at fixed distance from another point after too long


list of dumb things:
- still have single points floating? (probably due to triangle actually constructing a point)
- still have line translated where a segment already exists
- too often measure angles and rotate angles (but this is the "all" distribution").
- measure angle of three collinear points
- ask for property of thing that was simply constructed by reflection, e.g, mirror_cl
- ask for distance between two points when one point was rotated about the other
- ask for intersection of two lines when one was the perpendicular bisector of the other, so we're just asking for the midpoint

- add more commands(?)
    - fix measure angle for general problems.
        - eliminate angle category, rotate_by_equivalent_angle command
    - construct point at distance along line (???)
    - build triangle by fiat (return the points)
        - equilateral triangle
        - triangle with fixed lengths
        - triangle with fixed angle measures
            - too hard
    - circle chords
        - chord of fixed length
        - chord parallel to existing chord (maybe)
        [x] chord orthogonal to existing line (not possible)
    - internally tangent circle
    - externally tangent circle
    - circle tangent to two others? (has the problem of always returning two circles if they are externally tangent. but can do a randomizer.)
    [x] triangle area
    - check each of the 5 datasets for dumbness

- generally speaking, review problems

- sometimes things only rely in any meaningful way on something constructed way down the line
- note most functions only return one point, so this point should be meaningful in some way to the rest of the problem beforehand.
- functions which only take one input are possibly pretty bad?




- vim matters:
- ctrl-i, ctrl-n clash pretty badly with things i often want to do (composer, make new file)

- design a "compositional" geometry + "algebra" problem.
    - note the design constraints: 
    - the decomposition into two "subproblems" must be clear.
    - ideally, should make use of our existing capabilities to some extent...

- split exploratory geometry problems into categories for construction
[x] build the new geometry "visualization" problem dataset

[d] help design number theory problems/training set
    - a product of three primes property?

- guiding principle for design: try to make this thing a tool that actually is useful for generating geometry problems. maybe is useful in itself.


thinking out loud here:
- the thing is potentially useful to others, and impressive in itself
so we want to extend it to be general-purpose
- so we want to define a language to describe all the things you may want to specify in the constructions.
- not doing this is tantamount to asking the user to read the code and modify it themselves.
- but most importantly, I think we don't need to do this right now, particularly since it doesn't seem like there are very good solutions.

- uh, how are we gonna know if the distribution of questions is any good again?

think of uses for deep research and etc.


- generally improve sampling heuristics, as needed, for variety.
- would be good if we could somehow construct the set of points satisfying a predicate. but that seems quite advanced and somewhat out of the reach of this project...

## compositional problems
- need to make sure that the model can't solve the test questions.

-geo-arith problem: construct a regular polygon. set an arithmetic condition on when to draw lines between points
-subproblems: determine when the arithmetic condition holds. determine what the result of drawing the lines is:

-subproblems: be generally good at combinatorics / counting intersections and shit. be better at geometry in the sense that we know how to handle rotations and shit.

**************************************************************88
Work log pre-20250505
**************************************************************
list of dumb things:
[x] solve ident stuff (too many idents)
[x] lines could be constructed on top of each other, points, circles
[x] segment of fixed length is constructed, circle constructed with that as radius, we ask for the radius of the circle instead of just the length of the segment
    [x] now circles can't be constructed with segment, only by two points, three points, or radius directly
    [x] it's still possible to do dumb things like construct a circle with two points then ask for the radius, which is just the same as asking for the dist between the points. (filter constructions by conditions on the measured variable)
[x] likewise, it's possible to construct two points at a fixed distance and then just ask for the distance between those two points, which is kind of dumb.
    [x] note fundamentally there are only like three kinds of measures: distance, angle, area (special triangle ops like circumradius, inradius). and area is not really possible for anything other than a triangle because areas of regular polygons are boring and irregular polygons can't be reliably constructed. 
        [x] handled in translator measure type
        - it will be good to add "ratio".
            - will not do. too hard for too little reward
    [x] special casing for regular polygon translation

translations:
[x] a point constructed at a fixed distance from another point is just called a segment, so do that
    -instead of "A point G is placed so that it is 5 units away from point N in any direction." maybe  just "G is constructed so that NG = 5". Even worse if N was not constructed by important means, then it's just "NG = 5".

- distribution of problems. Triangles can be constructed more easily. Somehow still too many polygons are constructed


[d] generalize the parameterization logic for the exploratory geometry constructor.
    [d] see if all reasonable generators are actually just variants of the main one by params.
[x] move all the stuff into a single pipeline script
- and integrate with yiyou's repo
    [d] refactor the pipeline so that knowledge is carried forward. in particular, commands can easily be graded at construction time, and also translated directly from Command object. (Great opportunity for Cursor composer.)
        - this also is not actually needed. in particular, there is no benefit from abstracting out measure_test.py.
        [x] But the Construction parser can indeed be used for the translator, to derive types for everything. (Made the logic for translator incredibly simpler)
- improve the wording of problems to be more like AIME problems
    [x] read some AIME problems to get a sense of these
    - heuristics for operating on lines
    [x] rewording individual commands (particularly construction of individual points)

[x] reduce the frequency of construction of polygons.

[x] special case constructions of polygons and triangles in translation (handled automatically by ident logic)
    [x] simply ban polygon construction more than once
    [x] polygons are constructed at a fixed angle relative to the axes, which is a mistranslation. rotate polygon by a random amount.
    [x] the same general concept means functions that return more than one output have been banned
    [x] need to fix big random float angles

- improve problem quality:
    - look at the distribution of sampled commands more closely. some problems seem to be somewhat degenerate


**************************************************************88
Work log pre-20250430
**************************************************************
        -TO DERISK, GET TO READING SOME TRANSLATED PROBLEMS ASAP.
        - you can operationalize the structure by trying to write some problems by hand
        [x] initial behavior is quite simple: simply start by constructing a polygon, and soon thereafter invoke a rotation on the polygon.
        - tricky thing: should the rest of the diagram rotate as well? naturally, not, but then what is the nature of the relation to the original polygon, i.e, how do we ask about rotation?
        - maybe you want to refer to the original polygon's vertices, maybe sometimes you also want to refer to the "rotated" polygon's vertices. 
            [x] best trick for this is by making the idents variations on the originals, i.e, H gets rotated to H'.
            [x] also, sample sequential idents specifically for the polygon 80% of the time.
        [x] remember to sample interesting angles more often. override sample polygon sides or whatever it is, also
        - determine whether it makes sense to do axis-alignment / specify absolute slopes/orientation. i think probably not, in a pure sense.

- general critique: by tuning the difficulty of the problem we can appear to achieve a result, assuming the fine tuning helps at all and the difficulty is approximately projected onto one dimension.
if composition is an entirely separate matter from capability at the subproblems then fine.
    - resolution: problems actually need to broken into "hierarchy of problems" of increasing difficulty, so you know model capabilities and can measure it somehow.



**************************************************************88
Work log 20250425
**************************************************************
[x] why is openthinker so fricken slow? it actually creates problems for the pipeline
    - low hanging fruit: use_cache=False by default.
    - after that, no improvement seems possible; likely it is slower because the reasoning means a larger attention.
        - validated: token return speed is many, many times faster on shorter context, indicating that it's just the long attention calculations
        - trying to get around this: vllm has feature to batch preloads alongside decodes, which i thought would at least make use of the extra capacity when decodes get long, but the effect of this is basically negligible because the long decode compute times dominate completely.
    - re: trying to get the attention itself to run faster:
        - flashinfer is slower, even with other patches i found including FLASHINFER_FORCE_TENSOR_CORES which also does prebuilt kernels 
        - vllm docs say that pagedattention is used automatically, and also instrumentation automatically calculates the best attention backend for everything, so likely there is no fruit here
        - flash attention 3 is built into vllm main lib, based on looking at git and grepping the project for flash attention 3 implementation details
            - also already maximizes throughput for h100s and uses fp8 quantization whereever appropriate, so nothing to squeeze out there either?


**************************************************************88
Work log 20250425-2 (missing lots of debug logs... tired)
**************************************************************
stuck debugging:
[x] fix somehow making it to the end of all commands without sampling successfully (due to erroneous break in diagonal_p special logic)
[x] fix somehow getting stuck in infinite loop on rare occasion (due to chord construction infinite looping on degenerate circles; remove degen circle constructions)
[x] cannot reassign idents after polyogn is constructed, fixed: due to reusing const elements and the data field getting modified during polygon construction time
[x] polygons seem to be constructed at unreasonably high rates: not a bug, just higher than thought
[x] diagonal construction errors: could not map polygon to its vertices due to type error (polygon element vs polygon). type error in diagonal construction (segment_pp vs Segment constructor).

[x] type error in area_P
[x] insidious error affecting construction of polygons-- polygons would never make it because formed program was not semantically valid because number of sides changed due to sampling logic

[x] multithread the generator and the measure_test script
# 4, 7, 15. 30, 35, 48, 75, 94

-remember to check in translations:
    [x] angles/triangles/segments are not named
    [x] polygon vertices are used / polygons are involved in things other than simply the measure of their area
    [x] random_diagonal_p arise, and have sensible translations and the resulting points are actually used
    - rotate polygon_about_center is used sensibly

- handle
    [x] implement rotation around a vertex of a polygon, rotation around the center of a polygon.
    [x] implement constant generation logic for angle measures.
    [x] make a new constructor that overrides sample_command to start with.
**************************************************************88
Work log 20250424
**************************************************************
mechanical translator:

- now there are a couple of errors:
[x] polygons can be made with less than 3 sides i think
    [x] also improve polygon sampling generally
[x] some repetition of the word "segment", maybe "angle".
[x] angles should be specified to be measured in radians.

[x] also, reassign the idents, 'cause the existing problems are ass
[x] add specs to the generated problem jsons.
[x] encourage polygon_ppi() (this one is kind of tricky, maybe it shouldn't be done; maybe it should only happen towards the beginning)
[x] implement more commands: for triangles, to construct polygon from center with circumradius/vertex distance. polygon_ppi seems just to really not work.

fix the problem generator more:
- see what is up with all the probably-hard questions
- sample problems with o4 with a normal validation setup, and see what the approval rate is; 
- more robustly, it is probably possible to cross reference failed questions with the likeliest commands to cause failures, to find bugs in the commands. probably do that
**************************************************************88
Work log 20250423
**************************************************************

[x] create first draft (thankfully this was easy because i factored translate_to_nl really well in the first place)
- finish generating the basic dataset
	[x] prereq to everything: the classical generator problem
	- hardcode translator
		[x] handle const command constructions specially
	    [x] handle polygon command constructions specially
		[x] handle measure commands specially
		[x] check every single fucking command for syntactic correctness
			[x] check to see all the commands are there (polygon I think is not)
			[x] setup a split so that the process can be pipelined (find an example of command, look at command.py, look at translation)
        [x] bonus: improve wording of several specific constructions
		- bonus: do rewordings (and go through these manually, also)
- finish generating the non-basic dataset #1

[x] area takes variable inputs
[x] polygon takes variable inputs
[x] polygon_ppi has variable outputs
**************************************************************88
GRAVEYARD 20250423 (work prior to this date)
**************************************************************
[x] type annotate commands.py


objective is to make constructions randomly.
but pure randomness probably won't work.
[x] for starters, names for symbols should definitely not be random.
[x] secondly, it seems possible to automatically fit the syntax of the commands.
[x]thirdly, it seems like it should be possible to even make the commands type safe, ie, we can pick a command, we can look at the types of the arguments, and we can pick an argument from a list of options.

without worrying about the MC sampling part right now, assuming all the functions get correctly type annotated:
for each command get in code the types that it takes and then the types that it returns. 
for certain commands like poly, may need to do some annoying stuff like add extra args to construct a polygon.
also need to define a specific behavior for ints... 

a little heuristic needs to be done, also, maybe to see how things start, and/or to prune unused commands from files.
more generally, maybe ways needed to bias the sorts of commands that get called, so that the resulting commands are sensible. but maybe type checking will automatically do enough of this.
i think probably randomly constructing several ints and then pruning all the unused ints is going to work.
that, or, at this point since we're writing code, just redo the language so that int and float literals are possible.
defining the behavior of some commands with float literals might turn out to be annoying...

de-risk this by writing the original program generator with only a small set of commands, such as the ones used in some example files.

moreover, it seems like we should be able to do some sort of tree thing where we monte carlo sample to grow the construction tree,
maybe saving time in sampling if it takes too long to sample good constructions when the logic grows

some points of annoyance:
[x] need to also impose the constraint that different args are different identifiers, unless it's appropriate for them to be the same (check manually).
  [x] note i checked, i don't think it is ever very sensible to have the same identifier used for two different arguments in the same command.
[x] need to make sure that the measure command is always the last command in the construction (yes, this is done explicitly at the end.)
[x] idea for the numeric literals: construct them on demand in special cases, or start with very large pool of them if pruning goes quickly
[x] shouldn't we be seeing constructions of circles with a named radius? # This command exists, so we should sometimes.
[x] deleted the product commands for diversity, also removed a few other commands
[x] handled Union types and removed them from the set of commands.
[x] count num_ancestors in graph so that ones with high ancestors can be used
[x] keep track of types of things in graph so we can know to measure a measurable thing
[x] handle polygon() specially
[x] implement the algorithm that actually finds measurable nodes with a lot of ancestors. 


refactor:
[x] make sure retvals are handled in a sensible way.
[x] make sure that the return to sample_command is a sensible type.
[x] factor out the sampling logic so that can later be overridden.
[x] solve the polygon special case again.
[x] need to see if Element class or similar contains most of the logic for what we need for Node, or if Node can simply hold Element to reduce complexity.
[x] look at is_compatible_type(). is it still necessary? can't we just check type directly? only confusion is int and float, and generating new constants on the fly. this is already a bug.

- most ancestors calculation is currently heuristic. also, in general it is not clear that it gives you the characteristics that generate good constructions.
- generator does a bunch of work at initialization time. if too slow, measure it to reduce overhead.

- i think we want something slightly more sophisticated-- we want to measure something that requires a longer deduction at the end.
- *how do we ensure that most of the commands in the construction are actually necessary?* -- I think it should be possible to automatically prune unused commands from the construction.
  - note this may be achievable with a dependency graph; note this may not be strictly necessary because the pruning algorithm might be pretty easy.
- starting heuristics, constructing points and defining one numeric literal are too rigid. but i think good enough for now until other more obvious issues are solved.
