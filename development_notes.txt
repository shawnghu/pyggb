        -TO DERISK, GET TO READING SOME TRANSLATED PROBLEMS ASAP.
        - you can operationalize the structure by trying to write some problems by hand
        [x] initial behavior is quite simple: simply start by constructing a polygon, and soon thereafter invoke a rotation on the polygon.
        - tricky thing: should the rest of the diagram rotate as well? naturally, not, but then what is the nature of the relation to the original polygon, i.e, how do we ask about rotation?
        - maybe you want to refer to the original polygon's vertices, maybe sometimes you also want to refer to the "rotated" polygon's vertices. 
            [x] best trick for this is by making the idents variations on the originals, i.e, H gets rotated to H'.
            [x] also, sample sequential idents specifically for the polygon 80% of the time.
        [x] remember to sample interesting angles more often. override sample polygon sides or whatever it is, also
        - determine whether it makes sense to do axis-alignment / specify absolute slopes/orientation. i think probably not, in a pure sense.

    - ONLY after the initial run is done, you can override commands.py.

- note construction.render() function for visualizing the polygon intersection problems, later
- implement line crossing problems.

- later answer what yiyou wondered about whether o4 or r1 breaks first on these long problems


--REMEMBER to evaluate openthinker. it doesn't seem much better than qwen. 

- can improve feedback times on inference for small datasets by keeping a model loaded in memory using vllm server
    - vllm serve "open-thoughts/OpenThinker-32B" --tensor-parallel-size=1 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching

- generally improve sampling heuristics, as needed, for variety.
- would be good if we could somehow construct the set of points satisfying a predicate. but that seems quite advanced and somewhat out of the reach of this project...

## compositional problems
- need to make sure that the model can't solve the test questions.

-geo-arith problem: construct a regular polygon. set an arithmetic condition on when to draw lines between points
-subproblems: determine when the arithmetic condition holds. determine what the result of drawing the lines is:

-geo-combinatorics problem: aime dodecagon problem
-subproblems: be generally good at combinatorics / counting intersections and shit. be better at geometry in the sense that we know how to handle rotations and shit.

- general critique: by tuning the difficulty of the problem we can appear to achieve a result, assuming the fine tuning helps at all and the difficulty is approximately projected onto one dimension.
- if composition is an entirely separate matter from capability at the subproblems then fine.


**************************************************************88
Work log 20250425
**************************************************************
[x] why is openthinker so fricken slow? it actually creates problems for the pipeline
    - low hanging fruit: use_cache=False by default.
    - after that, no improvement seems possible; likely it is slower because the reasoning means a larger attention.
        - validated: token return speed is many, many times faster on shorter context, indicating that it's just the long attention calculations
        - trying to get around this: vllm has feature to batch preloads alongside decodes, which i thought would at least make use of the extra capacity when decodes get long, but the effect of this is basically negligible because the long decode compute times dominate completely.
    - re: trying to get the attention itself to run faster:
        - flashinfer is slower, even with other patches i found including FLASHINFER_FORCE_TENSOR_CORES which also does prebuilt kernels 
        - vllm docs say that pagedattention is used automatically, and also instrumentation automatically calculates the best attention backend for everything, so likely there is no fruit here
        - flash attention 3 is built into vllm main lib, based on looking at git and grepping the project for flash attention 3 implementation details
            - also already maximizes throughput for h100s and uses fp8 quantization whereever appropriate, so nothing to squeeze out there either?


**************************************************************88
Work log 20250425-2 (missing lots of debug logs... tired)
**************************************************************
stuck debugging:
[x] fix somehow making it to the end of all commands without sampling successfully (due to erroneous break in diagonal_p special logic)
[x] fix somehow getting stuck in infinite loop on rare occasion (due to chord construction infinite looping on degenerate circles; remove degen circle constructions)
[x] cannot reassign idents after polyogn is constructed, fixed: due to reusing const elements and the data field getting modified during polygon construction time
[x] polygons seem to be constructed at unreasonably high rates: not a bug, just higher than thought
[x] diagonal construction errors: could not map polygon to its vertices due to type error (polygon element vs polygon). type error in diagonal construction (segment_pp vs Segment constructor).

[x] type error in area_P
[x] insidious error affecting construction of polygons-- polygons would never make it because formed program was not semantically valid because number of sides changed due to sampling logic

[x] multithread the generator and the measure_test script
# 4, 7, 15. 30, 35, 48, 75, 94

-remember to check in translations:
    [x] angles/triangles/segments are not named
    [x] polygon vertices are used / polygons are involved in things other than simply the measure of their area
    [x] random_diagonal_p arise, and have sensible translations and the resulting points are actually used
    - rotate polygon_about_center is used sensibly

- handle
    [x] implement rotation around a vertex of a polygon, rotation around the center of a polygon.
    [x] implement constant generation logic for angle measures.
    [x] make a new constructor that overrides sample_command to start with.
**************************************************************88
Work log 20250424
**************************************************************
mechanical translator:

- now there are a couple of errors:
[x] polygons can be made with less than 3 sides i think
    [x] also improve polygon sampling generally
[x] some repetition of the word "segment", maybe "angle".
[x] angles should be specified to be measured in radians.

[x] also, reassign the idents, 'cause the existing problems are ass
[x] add specs to the generated problem jsons.
[x] encourage polygon_ppi() (this one is kind of tricky, maybe it shouldn't be done; maybe it should only happen towards the beginning)
[x] implement more commands: for triangles, to construct polygon from center with circumradius/vertex distance. polygon_ppi seems just to really not work.

fix the problem generator more:
- see what is up with all the probably-hard questions
- sample problems with o4 with a normal validation setup, and see what the approval rate is; 
- more robustly, it is probably possible to cross reference failed questions with the likeliest commands to cause failures, to find bugs in the commands. probably do that
**************************************************************88
Work log 20250423
**************************************************************

[x] create first draft (thankfully this was easy because i factored translate_to_nl really well in the first place)
- finish generating the basic dataset
	[x] prereq to everything: the classical generator problem
	- hardcode translator
		[x] handle const command constructions specially
	    [x] handle polygon command constructions specially
		[x] handle measure commands specially
		[x] check every single fucking command for syntactic correctness
			[x] check to see all the commands are there (polygon I think is not)
			[x] setup a split so that the process can be pipelined (find an example of command, look at command.py, look at translation)
        [x] bonus: improve wording of several specific constructions
		- bonus: do rewordings (and go through these manually, also)
- finish generating the non-basic dataset #1

[x] area takes variable inputs
[x] polygon takes variable inputs
[x] polygon_ppi has variable outputs
**************************************************************88
GRAVEYARD 20250423 (work prior to this date)
**************************************************************
[x] type annotate commands.py


objective is to make constructions randomly.
but pure randomness probably won't work.
[x] for starters, names for symbols should definitely not be random.
[x] secondly, it seems possible to automatically fit the syntax of the commands.
[x]thirdly, it seems like it should be possible to even make the commands type safe, ie, we can pick a command, we can look at the types of the arguments, and we can pick an argument from a list of options.

without worrying about the MC sampling part right now, assuming all the functions get correctly type annotated:
for each command get in code the types that it takes and then the types that it returns. 
for certain commands like poly, may need to do some annoying stuff like add extra args to construct a polygon.
also need to define a specific behavior for ints... 

a little heuristic needs to be done, also, maybe to see how things start, and/or to prune unused commands from files.
more generally, maybe ways needed to bias the sorts of commands that get called, so that the resulting commands are sensible. but maybe type checking will automatically do enough of this.
i think probably randomly constructing several ints and then pruning all the unused ints is going to work.
that, or, at this point since we're writing code, just redo the language so that int and float literals are possible.
defining the behavior of some commands with float literals might turn out to be annoying...

de-risk this by writing the original program generator with only a small set of commands, such as the ones used in some example files.

moreover, it seems like we should be able to do some sort of tree thing where we monte carlo sample to grow the construction tree,
maybe saving time in sampling if it takes too long to sample good constructions when the logic grows

some points of annoyance:
[x] need to also impose the constraint that different args are different identifiers, unless it's appropriate for them to be the same (check manually).
  [x] note i checked, i don't think it is ever very sensible to have the same identifier used for two different arguments in the same command.
[x] need to make sure that the measure command is always the last command in the construction (yes, this is done explicitly at the end.)
[x] idea for the numeric literals: construct them on demand in special cases, or start with very large pool of them if pruning goes quickly
[x] shouldn't we be seeing constructions of circles with a named radius? # This command exists, so we should sometimes.
[x] deleted the product commands for diversity, also removed a few other commands
[x] handled Union types and removed them from the set of commands.
[x] count num_ancestors in graph so that ones with high ancestors can be used
[x] keep track of types of things in graph so we can know to measure a measurable thing
[x] handle polygon() specially
[x] implement the algorithm that actually finds measurable nodes with a lot of ancestors. 


refactor:
[x] make sure retvals are handled in a sensible way.
[x] make sure that the return to sample_command is a sensible type.
[x] factor out the sampling logic so that can later be overridden.
[x] solve the polygon special case again.
[x] need to see if Element class or similar contains most of the logic for what we need for Node, or if Node can simply hold Element to reduce complexity.
[x] look at is_compatible_type(). is it still necessary? can't we just check type directly? only confusion is int and float, and generating new constants on the fly. this is already a bug.

- most ancestors calculation is currently heuristic. also, in general it is not clear that it gives you the characteristics that generate good constructions.
- generator does a bunch of work at initialization time. if too slow, measure it to reduce overhead.

- i think we want something slightly more sophisticated-- we want to measure something that requires a longer deduction at the end.
- *how do we ensure that most of the commands in the construction are actually necessary?* -- I think it should be possible to automatically prune unused commands from the construction.
  - note this may be achievable with a dependency graph; note this may not be strictly necessary because the pruning algorithm might be pretty easy.
- starting heuristics, constructing points and defining one numeric literal are too rigid. but i think good enough for now until other more obvious issues are solved.
